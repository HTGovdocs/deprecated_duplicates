require 'digest';
require 'htph';
require 'json';

# Take a list of files generated by general_marcreader.rb.
# Break into bits and put into the different hathi_xxx tables.
# If ARGV contains --delete, then wipe all the tables first.

@str_mem      = {};
@max_str_mem  = 10000;
@max_str_len  = 750;
@max_record_id_len = 50;
@str_mem_hit  = 0;
@str_mem_miss = 0;
@sha_digester = nil;

# Get a db connection, prep queries, get a digester and set up temporary loadfiles.
def setup ()
  db     = HTPH::Hathidb::Db.new();
  @conn  = db.get_conn();
  @bench = HTPH::Hathibench::Benchmark.new();
  last_id_sql         = "SELECT LAST_INSERT_ID() AS id";
  str_exist_sql       = "SELECT id, str FROM hathi_str WHERE str = ?";
  str_insert_sql      = "INSERT INTO hathi_str (str) VALUES (?)";
  hathi_gd_insert_sql = "INSERT INTO hathi_gd (gov_doc, file_id, lineno, mongo_id, hashsum, record_id, item_id) VALUES (?, ?, ?, ?, ?, ?, ?)";
  input_select_sql    = "SELECT id FROM hathi_input_file WHERE file_path = ?";
  input_insert_sql    = "INSERT INTO hathi_input_file (file_path, date_read) VALUES (?, SYSDATE())";

  @last_id_q         = @conn.prepare(last_id_sql);
  @str_exist_q       = @conn.prepare(str_exist_sql);
  @str_insert_q      = @conn.prepare(str_insert_sql);
  @hathi_gd_insert_q = @conn.prepare(hathi_gd_insert_sql);
  @input_select_q    = @conn.prepare(input_select_sql);
  @input_insert_q    = @conn.prepare(input_insert_sql);

  @sha_digester = Digest::SHA256.new();

  @loadfiles = {}; # Write tab-delim data, and when all is done, load into table.
  %w[isbn issn lccn oclc title enumc pubdate publisher sudoc].each do |suffix|
    @loadfiles[suffix] = HTPH::Hathidata::Data.new("#{suffix}.dat");
  end

  @infile_cache = {};
end

# Wipe tables before anything gets inserted, if that was specified in ARGV.
def delete
  %w[
    hathi_isbn
    hathi_issn
    hathi_lccn
    hathi_oclc
    hathi_title
    hathi_enumc
    hathi_pubdate
    hathi_publisher
    hathi_sudoc
    hathi_related
    hathi_gd
  ].each do |tablename|
    sql = "DELETE FROM #{tablename}";
    q   = @conn.prepare(sql);
    puts sql;
    q.execute();
  end
end

# Get a file id. Make one if file hasn't been seen before.
def get_infile_id (infile)
  if @infile_cache.has_key?(infile) then
    return @infile_cache[infile];
  end

  file_id = nil;
  @input_select_q.enumerate(infile) do |row|
    file_id = row[:id];
  end

  if file_id.nil? then
    @input_insert_q.execute(infile);
    @last_id_q.enumerate do |row|
      file_id = row[:id];
    end
  end

  if file_id.nil? then
    raise "No such infile [#{infile}] in cache #{@infile_cache.join(',')}";
  end

  @infile_cache[infile] = file_id;
  return file_id;
end

# Process a given input .ndj file.
def run (hdin)
  i    = 0;
  dups = 0;
  puts "Reading from #{hdin.path}";

  # Opens temporary loadfile. Overwrites existing file.
  @loadfiles.values.each do |hdout|
    hdout.open('w');
  end

  # Open infile and process line.
  hdin.open('r').file.each_line do |line|
    i += 1;

    # Skip stuff that doesn't look like json.
    if !line.start_with?('{') then
      STDERR.puts "Skipping line: #{line}";
      next;
      # For testing purposes.
      # elsif i > 2000 then
      #  puts "ok we are done here";
      #  break;
    elsif i % 1000 == 0 then
      puts "#{i} ...";
    end

    # gd_id is the id used as primary key in hathi_gd
    # and foreign key in all other tables, connecting the snowflake.
    gd_id     = nil;
    item_id   = nil;
    line_hash = JSON.parse(line);

    # The file where the data originally came from, not the file currently being read.
    infile    = line_hash['infile'];
    file_id   = get_infile_id(infile);
    # We don't want to include lineno or mongo_id in digest, so we delete them from the hash.
    lineno    = line_hash.delete('lineno');
    mongo_id  = line_hash.delete('mongo_id');
    # hashsum goes in hathi_gd to make sure we don't put total dups in there.
    hashsum   = @sha_digester.hexdigest(line_hash.to_json);

    # Not all records have a record id.
    rec_id    = 'N/A';
    if !line_hash['record_id'].nil? then
      rec_id  = line_hash['record_id'].first.values.first;

      if rec_id.length > @max_record_id_len then
        rec_id = rec_id[0..(@max_record_id_len-1)];
      end


    end

    if !line_hash['item_id'].nil? then
      if line_hash['item_id'].first.class == {}.class then
        item_id = line_hash['item_id'].first.values.first;
        if item_id.size > 50 then
          # Make sure we fit in the column.
          item_id = item_id[0..49];
        end
      end
    end

    # Get a gd_id from mysql.
    # Use it as primary key in hathi_gd.
    # Use it in all the tables (except hathi_str) as foreign key.
    begin
      @hathi_gd_insert_q.execute(1, file_id, lineno, mongo_id, hashsum, rec_id, item_id);
      @last_id_q.query() do |row|
        gd_id = row[:id];
      end
    rescue Java::ComMysqlJdbcExceptionsJdbc4::MySQLIntegrityConstraintViolationException => e
      if (e.to_s =~ /Duplicate entry.+for key 'hashsum'/) == 0 then
        dups += 1;
        next;
      else
        puts e;
        puts line;
      end
    end
    # If we got a gd_id, proceed to insert the rest.
    insert_line(line_hash, gd_id);
  end
  hdin.close();

  # When all the lines in the file have been read and the loadfiles are done,
  # use the loadfiles for their intended purpose.
  @loadfiles.keys.each do |suffix|
    loadfile = @loadfiles[suffix];
    loadfile.close();
    sql = "LOAD DATA LOCAL INFILE ? REPLACE INTO TABLE hathi_#{suffix} (gd_id, str_id, marc_field)";
    puts sql;
    query = @conn.prepare(sql);
    query.execute(loadfile.path);
    # loadfile.delete();
  end

  puts @bench.prettyprint();
  puts "#{dups} dups";
end

# writes lines of gd_id<tab>str_id<tab>marc_field to the loadfiles based on the values in the json hash.
def insert_line (json, gd_id)
  json.default = [];

  # Get the oclc value, look up a str_id for it and write to oclc load file together with the gd_id and marc_field.
  json['oclc'].each do |oclc|
    marc_field = oclc.keys.first;
    val        = oclc[marc_field].to_s;
    next if val.empty?;
    str_id     = get_str_id(val);
    @loadfiles['oclc'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  # Get the sudoc value, look up a str_id for it and write to sudoc load file together with the gd_id and marc_field.
  json['sudoc'].each do |sudoc|
    marc_field = sudoc.keys.first;
    val        = HTPH::Hathinormalize.sudoc(sudoc[marc_field].to_s);
    next if val.nil?;
    next if val.empty?;
    str_id     = get_str_id(val);
    @loadfiles['sudoc'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  # ... and so on for isbn, issn, lccn, title, enumc, pubdate and publisher.
  json['isbn'].each do |isbn|
    marc_field = isbn.keys.first;
    val        = isbn[marc_field].to_s;
    next if val.empty?;
    str_id     = get_str_id(val);
    @loadfiles['isbn'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  json['issn'].each do |issn|
    marc_field = issn.keys.first;
    val        = issn[marc_field].to_s;
    next if val.empty?;
    next if val == '1'; # Common crud. Perhaps no longer.

    str_id = get_str_id(val);
    @loadfiles['issn'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  json['lccn'].each do |lccn|
    marc_field = lccn.keys.first;
    val        = lccn[marc_field].to_s;
    next if val.empty?;
    str_id     = get_str_id(val);
    @loadfiles['lccn'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  json['title'].each do |title|
    marc_field = title.keys.first;
    val        = HTPH::Hathinormalize.title(title[marc_field]);
    next if val.nil?;
    next if val.empty?;
    str_id = get_str_id(val);
    @loadfiles['title'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  json['enumc'].each do |enumc|
    marc_field = enumc.keys.first;
    val        = HTPH::Hathinormalize.enumc(enumc[marc_field]);
    next if val.nil?;
    next if val.empty?;
    str_id = get_str_id(val);
    @loadfiles['enumc'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  json['pubdate'].each do |pubdate|
    marc_field = pubdate.keys.first;
    val        = pubdate[marc_field].to_s;
    # Date normalization?
    next if val.nil?
    next if val.empty?;
    str_id     = get_str_id(val);
    @loadfiles['pubdate'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

  json['publisher'].each do |publisher|
    marc_field = publisher.keys.first;
    val        = HTPH::Hathinormalize.agency(publisher[marc_field]);
    next if val.nil?;
    next if val.empty?;
    str_id = get_str_id(val);
    @loadfiles['publisher'].file.puts("#{gd_id}\t#{str_id}\t#{marc_field}");
  end

end

# Instead of always storing the string, store a string id.
# Get this from the hathi_str table, which keeps track of
# which string has which id. Use an internal hash as cache,
# so that e.g. if we are in a stretch of documents with the
# same sudoc number and publisher we don't have to hit the
# database each time.
def get_str_id (str)
  str_id = nil;
  # Trim spaces and truncate if necessary.
  str = str.gsub(/ +/, ' ');
  str = str.sub(/^ /, '');
  str = str.sub(/ $/, '');
  str = str[0..(@max_str_len-1)];

  if str == '' then
    return str_id;
  end

  # Check cache and return if found.
  if @str_mem.has_key?(str) then
    @str_mem_hit += 1;
    return @str_mem[str];
  end
  @str_mem_miss += 1;

  # Else look up in hathi_str.
  @str_exist_q.enumerate(str) do |res|
    str_id = res[:id];
  end

  # If not found in hathi_str, insert it.
  if str_id.nil? then
    @bench.time('insert_str') do
      @str_insert_q.execute(str);
    end
    @last_id_q.enumerate do |res|
      str_id = res[:id];
    end
  end

  # Make sure cache doesn't grow out of hand.
  if @str_mem.keys.size >= @max_str_mem then
    @bench.time('str_mem') do
      # Mem hash is full, make some room, delete first 10% of keys.
      @str_mem.keys[0 .. (@str_mem.keys.size / 10)].each do |k|
        @str_mem.delete(k);
      end
    end
  end

  # Update cache if cache miss.
  @str_mem[str] = str_id.to_i;

  return str_id.to_i;
end

if __FILE__ == $0 then
  setup();

  ARGV.map!{|arg|
    # Check if we're supposed to wipe the tables first.
    (arg != '--delete' && arg) || delete() && nil;
  }.compact!

  if ARGV.size <= 0 then
    raise "Need infile as 1st arg.";
  end

  # Process each file given in ARGV.
  while  ARGV.size > 0 do
    infile = ARGV.shift;
    hdin   = HTPH::Hathidata::Data.new(infile);
    if !hdin.exists? then
      raise "Cannot find infile #{hdin.path}.";
    end

    run(hdin);
  end
end
